{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['description', 'data', 'target', 'target_names', 'DESCR', 'filenames'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스 분류기 모델 만들기\n",
    "\n",
    "* 데이터 파악\n",
    "* 전처리(Preprocessing)\n",
    "\n",
    "    * 필요없는 단어 제거 (Data Cleansing)\n",
    "    * CountVectorizer & Tf-idfVectorizer\n",
    "\n",
    "---\n",
    "    \n",
    "* Modeling : BernoulliNB, MultinomialNB 사용\n",
    "  * Cross Validation(Kfold 이용)\n",
    "  \n",
    "---\n",
    "\n",
    "* Pipeline 이용\n",
    "\n",
    "---\n",
    "\n",
    "* Assignment Description\n",
    "     * 위 신문 데이터를 바탕으로 신문 내용별 분류기를 개발하라\n",
    "     * 위 데이터를 Traing / Test Dataset으로 나눠서 5-fold cross validation(5번 데이터를 training / testset으로 나눔, KV 활용)\n",
    "     * Naive Bayesian Classifier와 Count Vector를 활용하여 각각 성능을 테스트하라\n",
    "         * NB는 multinomial과 bernuoil 분포를 모두 사용하라\n",
    "     * 가능할 경우, TF-IDF vector를 활용해 볼것 (검색어 - tf-idf scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "   * 18846개의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'News' : news.data, 'Target' : news.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News  Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...      10\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...       3\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...      17\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...       3\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...       4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...          rec.sport.hockey\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...  comp.sys.ibm.pc.hardware\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...     talk.politics.mideast\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...  comp.sys.ibm.pc.hardware\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target 데이터 -> 문자 라벨링(뉴스마다 어떤 뉴스인지 보기 편하도록 만들기 위해서)\n",
    "def word_labeling(lst, df):\n",
    "    for idx, name in enumerate(lst):\n",
    "        target_data = df['Target']\n",
    "        for idx_, num_label in enumerate(target_data):\n",
    "            if num_label == idx:\n",
    "                df.loc[idx_, 'Target'] = name\n",
    "    return df\n",
    "news_df = word_labeling(news['target_names'], news_df)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Cleansing\n",
    "    * 이메일 제거\n",
    "    * 불필요 숫자 제거\n",
    "    * 문자 아닌 특수문자 제거\n",
    "    * 단어 사이 공백 제거 : 띄어쓰기 별로 split해주고 join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(df):\n",
    "    delete_email = re.sub(r'\\b[\\w\\+]+@[\\w]+.[\\w]+.[\\w]+.[\\w]+\\b', ' ', df)\n",
    "    delete_number = re.sub(r'\\b|\\d+|\\b', ' ',delete_email)\n",
    "    delete_non_word = re.sub(r'\\b[\\W]+\\b', ' ', delete_number)\n",
    "    cleaning_result = ' '.join(delete_non_word.split())\n",
    "    return cleaning_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From Mamatha Devineni Ratnam Subject Pens fans...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Matthew B Lawson Subject Which high perfo...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Guy Dawson Subject Re IDE vs SCSI DMA and...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From Alexander Samuel McDiarmid Subject driver...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                News                    Target\n",
       "0  From Mamatha Devineni Ratnam Subject Pens fans...          rec.sport.hockey\n",
       "1  From Matthew B Lawson Subject Which high perfo...  comp.sys.ibm.pc.hardware\n",
       "2  From hilmi Hilmi Eren Subject Re ARMENIA SAYS ...     talk.politics.mideast\n",
       "3  From Guy Dawson Subject Re IDE vs SCSI DMA and...  comp.sys.ibm.pc.hardware\n",
       "4  From Alexander Samuel McDiarmid Subject driver...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.loc[:, 'News'] = news_df['News'].apply(data_cleansing)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer\n",
    "* CountVectorizer \n",
    "  * 문서 집합으로부터 단어의 수를 세어 카운트 행렬을 만듦\n",
    "* TfidfVectorizer \n",
    "    * 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법\n",
    "    * TF(Term Frequency) : 문서에서 해당 단어가 얼마나 나왔는지 나타내주는 빈도 수\n",
    "    * DF(Document Frequency) : 해당 단어가 있는 문서의 수\n",
    "    * IDF(Inverse Document Frequency) 해당 단어가 있는 문서의 수가 높아질 수록 가중치를 축소해주기 위해 역수 취해줌\n",
    "        * log(N / (1 + DF))      \n",
    "            * N : 전체 문서의 수\n",
    "    * TF-IDF = TF * IDF\n",
    "* CustomizedVectorizer - StemmedCounterVectorizer, StemmedTfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', 'look', 'look']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import stem\n",
    "stmmer = stem.SnowballStemmer(\"english\")\n",
    "sentence = 'looking looks looked'\n",
    "[stmmer.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('imag', 'imag', 'imagin')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmmer.stem(\"images\"), stmmer.stem(\"imaging\"), stmmer.stem(\"imagination\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/pip\r\n"
     ]
    }
   ],
   "source": [
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import  nltk\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'look': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StemmedCountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'looked': 0, 'looking': 1, 'looks': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorizer(min_df=1, stop_words=\"english\").fit([sentence]).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "enlish_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "* Pipeline\n",
    "* Gridsearch\n",
    "* Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'foo')\n",
      "('a', 'foo', 'bar')\n",
      "('foo', 'bar', 'sentences')\n",
      "('bar', 'sentences', 'and')\n",
      "('sentences', 'and', 'i')\n",
      "('and', 'i', 'want')\n",
      "('i', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), 3)\n",
    "for grams in sixgrams:\n",
    "  print (grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pipeline(memory=None,\n",
       "      steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "   ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...   vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedcountvectorizer', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), preprocessor...e, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedcountvectorizer', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), preprocessor...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), norm='l2', p...e, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       " Pipeline(memory=None,\n",
       "      steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "             dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "             lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "             ngram_range=(1, 1), norm='l2', p...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False))])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB,GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer(), StemmedCountVectorizer(), StemmedTfidfVectorizer()]\n",
    "# algorithms = [BernoulliNB(), MultinomialNB(), GaussianNB(), LogisticRegression()]\n",
    "algorithms = [MultinomialNB(), LogisticRegression()]\n",
    "\n",
    "pipelines  = [] \n",
    "\n",
    "\n",
    "import itertools\n",
    "for case in list(itertools.product(vectorizer, algorithms)):\n",
    "    if isinstance(case[1], GaussianNB):\n",
    "        case = list(case)\n",
    "        case.insert(1,  DenseTransformer())\n",
    "    pipelines.append(make_pipeline(*case))\n",
    "pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer Common params\n",
    "ngrams_params = [(1,1),(1,3)]\n",
    "stopword_params = [\"english\"]\n",
    "lowercase_params = [True, False]\n",
    "max_df_params = np.linspace(0.4, 0.6, num=6)\n",
    "min_df_params = np.linspace(0.0, 0.0, num=1)\n",
    "\n",
    "attributes = {\"ngram_range\":ngrams_params, \"max_df\":max_df_params,\"min_df\":min_df_params,\n",
    "              \"lowercase\":lowercase_params,\"stop_words\":stopword_params}\n",
    "vectorizer_names = [\"countvectorizer\",\"tfidfvectorizer\",\"stemmedcountvectorizer\",\"stemmedtfidfvectorizer\"]\n",
    "vectorizer_params_dict = {}\n",
    "\n",
    "for vect_name in vectorizer_names:\n",
    "    vectorizer_params_dict[vect_name] = {}\n",
    "    for key, value in attributes.items():\n",
    "        param_name = vect_name + \"__\" + key\n",
    "        vectorizer_params_dict[vect_name][param_name] =  value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression': [{'logisticregression__C': [0.1,\n",
       "    5.0,\n",
       "    7.0,\n",
       "    10.0,\n",
       "    15.0,\n",
       "    20.0,\n",
       "    100.0],\n",
       "   'logisticregression__multi_class': ['multinomial'],\n",
       "   'logisticregression__penalty': ['l1'],\n",
       "   'logisticregression__solver': ['saga']},\n",
       "  {'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear']}],\n",
       " 'multinomialnb': {'multinomialnb__alpha': array([ 1.])}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Algorithms parameters\n",
    "algorithm_names = [\"bernoullinb\",\"multinomialnb\",\"gaussiannb\",\"logisticregression\"]\n",
    "algorithm_names = [\"multinomialnb\", \"logisticregression\"]\n",
    "\n",
    "algorithm_params_dict = {}\n",
    "\n",
    "\n",
    "#'bernoullinb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))])\n",
    "alpha_params = np.linspace(1.0, 1.0, num=1)\n",
    "for i in range(1):\n",
    "    algorithm_params_dict[algorithm_names[i]] = {\n",
    "    algorithm_names[i]+ \"__alpha\" : alpha_params    \n",
    "    }\n",
    "# algorithm_params_dict[algorithm_names[2]] = {}\n",
    "\n",
    "\n",
    "# LogisticRegression    \n",
    "# multi_class : str, {‘ovr’, ‘multinomial’}, default: ‘ovr’\n",
    "# C : float, default: 1.0\n",
    "# solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "# n_jobs : int, default: 1\n",
    "# penalty : str, ‘l1’ or ‘l2’, default: ‘l2’\n",
    "\n",
    "# multi_class_params = [\"ovr\", \"multinomial\"]\n",
    "c_params = [0.1,  5.0, 7.0, 10.0, 15.0, 20.0, 100.0]\n",
    "\n",
    "\n",
    "\n",
    "algorithm_params_dict[algorithm_names[1]] = [{\n",
    "    \"logisticregression__multi_class\" : [\"multinomial\"],\n",
    "    \"logisticregression__solver\" : [\"saga\"],\n",
    "    \"logisticregression__penalty\" : [\"l1\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    },{\n",
    "    \"logisticregression__multi_class\" : [\"ovr\"],\n",
    "    \"logisticregression__solver\" : ['liblinear'],\n",
    "    \"logisticregression__penalty\" : [\"l2\"],\n",
    "    \"logisticregression__C\" : c_params\n",
    "    }\n",
    "    ]\n",
    "algorithm_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'countvectorizer__lowercase': [True, False],\n",
       "  'countvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "  'countvectorizer__min_df': array([ 0.]),\n",
       "  'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'countvectorizer__stop_words': ['english'],\n",
       "  'multinomialnb__alpha': array([ 1.])},\n",
       " [{'countvectorizer__lowercase': [True, False],\n",
       "   'countvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'countvectorizer__min_df': array([ 0.]),\n",
       "   'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'countvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear']},\n",
       "  {'countvectorizer__lowercase': [True, False],\n",
       "   'countvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'countvectorizer__min_df': array([ 0.]),\n",
       "   'countvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'countvectorizer__stop_words': ['english'],\n",
       "   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear']}],\n",
       " {'multinomialnb__alpha': array([ 1.]),\n",
       "  'tfidfvectorizer__lowercase': [True, False],\n",
       "  'tfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "  'tfidfvectorizer__min_df': array([ 0.]),\n",
       "  'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'tfidfvectorizer__stop_words': ['english']},\n",
       " [{'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'tfidfvectorizer__lowercase': [True, False],\n",
       "   'tfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'tfidfvectorizer__min_df': array([ 0.]),\n",
       "   'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'tfidfvectorizer__stop_words': ['english']},\n",
       "  {'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'tfidfvectorizer__lowercase': [True, False],\n",
       "   'tfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'tfidfvectorizer__min_df': array([ 0.]),\n",
       "   'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'tfidfvectorizer__stop_words': ['english']}],\n",
       " {'multinomialnb__alpha': array([ 1.]),\n",
       "  'stemmedcountvectorizer__lowercase': [True, False],\n",
       "  'stemmedcountvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "  'stemmedcountvectorizer__min_df': array([ 0.]),\n",
       "  'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedcountvectorizer__stop_words': ['english']},\n",
       " [{'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'stemmedcountvectorizer__lowercase': [True, False],\n",
       "   'stemmedcountvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'stemmedcountvectorizer__min_df': array([ 0.]),\n",
       "   'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedcountvectorizer__stop_words': ['english']},\n",
       "  {'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'stemmedcountvectorizer__lowercase': [True, False],\n",
       "   'stemmedcountvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'stemmedcountvectorizer__min_df': array([ 0.]),\n",
       "   'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedcountvectorizer__stop_words': ['english']}],\n",
       " {'multinomialnb__alpha': array([ 1.]),\n",
       "  'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "  'stemmedtfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "  'stemmedtfidfvectorizer__min_df': array([ 0.]),\n",
       "  'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'stemmedtfidfvectorizer__stop_words': ['english']},\n",
       " [{'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "   'stemmedtfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'stemmedtfidfvectorizer__min_df': array([ 0.]),\n",
       "   'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedtfidfvectorizer__stop_words': ['english']},\n",
       "  {'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0],\n",
       "   'logisticregression__multi_class': ['ovr'],\n",
       "   'logisticregression__penalty': ['l2'],\n",
       "   'logisticregression__solver': ['liblinear'],\n",
       "   'stemmedtfidfvectorizer__lowercase': [True, False],\n",
       "   'stemmedtfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]),\n",
       "   'stemmedtfidfvectorizer__min_df': array([ 0.]),\n",
       "   'stemmedtfidfvectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'stemmedtfidfvectorizer__stop_words': ['english']}]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_params= []\n",
    "for case in list(itertools.product(vectorizer_names, algorithm_names)):\n",
    "    vect_params = vectorizer_params_dict[case[0]].copy()\n",
    "    algo_params = algorithm_params_dict[case[1]]\n",
    "    \n",
    "    if isinstance(algo_params, dict):\n",
    "        vect_params.update(algo_params)\n",
    "        pipeline_params.append(vect_params)\n",
    "    else:\n",
    "        temp = []\n",
    "        for param in algo_params:\n",
    "            vect_params.update(param)\n",
    "            temp.append(vect_params)\n",
    "        pipeline_params.append(temp)\n",
    "pipeline_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn! Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_data = news_df.loc[:, 'News'].tolist()\n",
    "y_data = news_df['Target'].tolist()\n",
    "y = LabelEncoder().fit_transform(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), p..., vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid={'countvectorizer__stop_words': ['english'], 'multinomialnb__alpha': array([ 1.]), 'countvectorizer__ngram_range': [(1, 1), (1, 3)], 'countvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]), 'countvectorizer__lowercase': [True, False], 'countvectorizer__min_df': array([ 0.])},\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 120 out of 120 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid=[{'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0], 'logisticregression__solver': ['liblinear'], 'logisticregression__multi_class': ['ovr'], 'logisticregression__penalty': ['l2'], 'countvectorizer__stop_words': ['english'], 'countvectorizer__ngram_range': [(1, 1), (1, 3)]....56,  0.6 ]), 'countvectorizer__lowercase': [True, False], 'countvectorizer__min_df': array([ 0.])}],\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 128 tasks      | elapsed: 85.8min\n",
      "[Parallel(n_jobs=36)]: Done 378 tasks      | elapsed: 223.1min\n",
      "[Parallel(n_jobs=36)]: Done 728 tasks      | elapsed: 448.5min\n",
      "[Parallel(n_jobs=36)]: Done 1178 tasks      | elapsed: 750.6min\n",
      "[Parallel(n_jobs=36)]: Done 1680 out of 1680 | elapsed: 1070.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...   vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid={'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)], 'tfidfvectorizer__stop_words': ['english'], 'tfidfvectorizer__min_df': array([ 0.]), 'multinomialnb__alpha': array([ 1.]), 'tfidfvectorizer__lowercase': [True, False], 'tfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ])},\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 120 out of 120 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid=[{'tfidfvectorizer__min_df': array([ 0.]), 'logisticregression__penalty': ['l2'], 'tfidfvectorizer__lowercase': [True, False], 'logisticregression__multi_class': ['ovr'], 'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)], 'tfidfvectorizer__stop_words': ['english'], 'logisticregression__C':...lver': ['liblinear'], 'tfidfvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ])}],\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 128 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=36)]: Done 378 tasks      | elapsed: 51.2min\n",
      "[Parallel(n_jobs=36)]: Done 728 tasks      | elapsed: 112.3min\n",
      "[Parallel(n_jobs=36)]: Done 1178 tasks      | elapsed: 178.7min\n",
      "[Parallel(n_jobs=36)]: Done 1680 out of 1680 | elapsed: 269.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('stemmedcountvectorizer', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor...e, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid={'multinomialnb__alpha': array([ 1.]), 'stemmedcountvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]), 'stemmedcountvectorizer__lowercase': [True, False], 'stemmedcountvectorizer__stop_words': ['english'], 'stemmedcountvectorizer__min_df': array([ 0.]), 'stemmedcountvectorizer__ngram_range': [(1, 1), (1, 3)]},\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 120 out of 120 | elapsed: 22.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('stemmedcountvectorizer', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "       fit_params=None, iid=True, n_jobs=36,\n",
      "       param_grid=[{'stemmedcountvectorizer__max_df': array([ 0.4 ,  0.44,  0.48,  0.52,  0.56,  0.6 ]), 'stemmedcountvectorizer__lowercase': [True, False], 'stemmedcountvectorizer__stop_words': ['english'], 'logisticregression__solver': ['liblinear'], 'logisticregression__multi_class': ['ovr'], 'stemmedco...sion__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0], 'stemmedcountvectorizer__min_df': array([ 0.])}],\n",
      "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
      "       return_train_score='warn', scoring=['accuracy'], verbose=1)\n",
      "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "scoring = ['accuracy']\n",
    "estimator_results = []\n",
    "for i, (estimator, params) in enumerate(zip(pipelines,pipeline_params)):\n",
    "    n_jobs = 36\n",
    "#     if i+1 % 3 == 0:\n",
    "#         n_jobs = 2\n",
    "    gs_estimator = GridSearchCV(\n",
    "            refit=\"accuracy\", estimator=estimator,param_grid=params, scoring=scoring, cv=5, verbose=1, n_jobs=n_jobs)\n",
    "    print(gs_estimator)\n",
    "\n",
    "    gs_estimator.fit(X_data, y)\n",
    "    estimator_results.append(gs_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>alpha</th>\n",
       "      <th>binarize</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>model</th>\n",
       "      <th>ngram_rangemulti_class</th>\n",
       "      <th>penalty</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>solver</th>\n",
       "      <th>vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C accuracy alpha binarize lowercase max_df min_df model  \\\n",
       "0   None     None  None     None      None   None   None  None   \n",
       "1   None     None  None     None      None   None   None  None   \n",
       "2   None     None  None     None      None   None   None  None   \n",
       "3   None     None  None     None      None   None   None  None   \n",
       "4   None     None  None     None      None   None   None  None   \n",
       "5   None     None  None     None      None   None   None  None   \n",
       "6   None     None  None     None      None   None   None  None   \n",
       "7   None     None  None     None      None   None   None  None   \n",
       "8   None     None  None     None      None   None   None  None   \n",
       "9   None     None  None     None      None   None   None  None   \n",
       "10  None     None  None     None      None   None   None  None   \n",
       "11  None     None  None     None      None   None   None  None   \n",
       "12  None     None  None     None      None   None   None  None   \n",
       "13  None     None  None     None      None   None   None  None   \n",
       "14  None     None  None     None      None   None   None  None   \n",
       "15  None     None  None     None      None   None   None  None   \n",
       "\n",
       "   ngram_rangemulti_class penalty precision_macro recall_macro solver  \\\n",
       "0                    None    None            None         None   None   \n",
       "1                    None    None            None         None   None   \n",
       "2                    None    None            None         None   None   \n",
       "3                    None    None            None         None   None   \n",
       "4                    None    None            None         None   None   \n",
       "5                    None    None            None         None   None   \n",
       "6                    None    None            None         None   None   \n",
       "7                    None    None            None         None   None   \n",
       "8                    None    None            None         None   None   \n",
       "9                    None    None            None         None   None   \n",
       "10                   None    None            None         None   None   \n",
       "11                   None    None            None         None   None   \n",
       "12                   None    None            None         None   None   \n",
       "13                   None    None            None         None   None   \n",
       "14                   None    None            None         None   None   \n",
       "15                   None    None            None         None   None   \n",
       "\n",
       "   vectorizer  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "5        None  \n",
       "6        None  \n",
       "7        None  \n",
       "8        None  \n",
       "9        None  \n",
       "10       None  \n",
       "11       None  \n",
       "12       None  \n",
       "13       None  \n",
       "14       None  \n",
       "15       None  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "result_df_dict = {}\n",
    "result_attributes = [\"vectorizer\", \"model\", \"accuracy\", \"recall_macro\",\"precision_macro\" , \"min_df\", \n",
    "                     \"lowercase\", \"max_df\", \"binarize\", \"alpha\", \"ngram_range\"\n",
    "                     \"multi_class\", \"penalty\", \"solver\", \"C\"]\n",
    "\n",
    "pieline_list =  list(itertools.product(vectorizer_names, algorithm_names))\n",
    "\n",
    "for att in result_attributes:\n",
    "    result_df_dict[att] = [None for i in range(16)]\n",
    "\n",
    "result_df = DataFrame(result_df_dict)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer', 'multinomialnb'),\n",
       " ('countvectorizer', 'logisticregression'),\n",
       " ('tfidfvectorizer', 'multinomialnb'),\n",
       " ('tfidfvectorizer', 'logisticregression'),\n",
       " ('stemmedcountvectorizer', 'multinomialnb'),\n",
       " ('stemmedcountvectorizer', 'logisticregression'),\n",
       " ('stemmedtfidfvectorizer', 'multinomialnb'),\n",
       " ('stemmedtfidfvectorizer', 'logisticregression')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pieline_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, estiamtor in enumerate(estimator_results):\n",
    "    best_estimator = estiamtor.best_estimator_\n",
    "    best_index = estiamtor.best_index_\n",
    "    result_df_dict[\"vectorizer\"][i] = pieline_list[i][0]\n",
    "    result_df_dict[\"model\"][i] = pieline_list[i][1]\n",
    "    result_df_dict[\"accuracy\"][i] = estiamtor.best_score_\n",
    "#     result_df_dict[\"recall_micro\"][i] = estiamtor.cv_results_[\"mean_test_recall_micro\"][best_index]\n",
    "#     result_df_dict[\"precision_micro\"][i] = estiamtor.cv_results_[\"mean_test_precision_micro\"][best_index]\n",
    "    for key, value in estiamtor.best_params_.items():\n",
    "        if key.split(\"__\")[1] in result_df_dict:\n",
    "            name = key.split(\"__\")[1]\n",
    "            result_df_dict[key.split(\"__\")[1]][i] = value\n",
    "#     print(estiamtor.best_params_)\n",
    "#     print(a.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>min_df</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>max_df</th>\n",
       "      <th>binarize</th>\n",
       "      <th>alpha</th>\n",
       "      <th>ngram_rangemulti_class</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stemmedtfidfvectorizer</td>\n",
       "      <td>logisticregression</td>\n",
       "      <td>0.936432</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.52</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidfvectorizer</td>\n",
       "      <td>logisticregression</td>\n",
       "      <td>0.936061</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>countvectorizer</td>\n",
       "      <td>logisticregression</td>\n",
       "      <td>0.921097</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.40</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stemmedcountvectorizer</td>\n",
       "      <td>logisticregression</td>\n",
       "      <td>0.920885</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.40</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>countvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.908893</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stemmedcountvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.906240</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidfvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.903056</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.44</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stemmedtfidfvectorizer</td>\n",
       "      <td>multinomialnb</td>\n",
       "      <td>0.900934</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.40</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                vectorizer               model  accuracy recall_macro  \\\n",
       "7   stemmedtfidfvectorizer  logisticregression  0.936432         None   \n",
       "3          tfidfvectorizer  logisticregression  0.936061         None   \n",
       "1          countvectorizer  logisticregression  0.921097         None   \n",
       "5   stemmedcountvectorizer  logisticregression  0.920885         None   \n",
       "0          countvectorizer       multinomialnb  0.908893         None   \n",
       "4   stemmedcountvectorizer       multinomialnb  0.906240         None   \n",
       "2          tfidfvectorizer       multinomialnb  0.903056         None   \n",
       "6   stemmedtfidfvectorizer       multinomialnb  0.900934         None   \n",
       "8                     None                None       NaN         None   \n",
       "9                     None                None       NaN         None   \n",
       "10                    None                None       NaN         None   \n",
       "11                    None                None       NaN         None   \n",
       "12                    None                None       NaN         None   \n",
       "13                    None                None       NaN         None   \n",
       "14                    None                None       NaN         None   \n",
       "15                    None                None       NaN         None   \n",
       "\n",
       "   precision_macro  min_df lowercase  max_df binarize  alpha  \\\n",
       "7             None     0.0     False    0.52     None    NaN   \n",
       "3             None     0.0      True    0.44     None    NaN   \n",
       "1             None     0.0      True    0.40     None    NaN   \n",
       "5             None     0.0     False    0.40     None    NaN   \n",
       "0             None     0.0      True    0.44     None    1.0   \n",
       "4             None     0.0     False    0.44     None    1.0   \n",
       "2             None     0.0      True    0.44     None    1.0   \n",
       "6             None     0.0     False    0.40     None    1.0   \n",
       "8             None     NaN      None     NaN     None    NaN   \n",
       "9             None     NaN      None     NaN     None    NaN   \n",
       "10            None     NaN      None     NaN     None    NaN   \n",
       "11            None     NaN      None     NaN     None    NaN   \n",
       "12            None     NaN      None     NaN     None    NaN   \n",
       "13            None     NaN      None     NaN     None    NaN   \n",
       "14            None     NaN      None     NaN     None    NaN   \n",
       "15            None     NaN      None     NaN     None    NaN   \n",
       "\n",
       "   ngram_rangemulti_class penalty     solver      C  \n",
       "7                    None      l2  liblinear  100.0  \n",
       "3                    None      l2  liblinear  100.0  \n",
       "1                    None      l2  liblinear   15.0  \n",
       "5                    None      l2  liblinear   20.0  \n",
       "0                    None    None       None    NaN  \n",
       "4                    None    None       None    NaN  \n",
       "2                    None    None       None    NaN  \n",
       "6                    None    None       None    NaN  \n",
       "8                    None    None       None    NaN  \n",
       "9                    None    None       None    NaN  \n",
       "10                   None    None       None    NaN  \n",
       "11                   None    None       None    NaN  \n",
       "12                   None    None       None    NaN  \n",
       "13                   None    None       None    NaN  \n",
       "14                   None    None       None    NaN  \n",
       "15                   None    None       None    NaN  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = DataFrame(result_df_dict, columns=result_attributes)\n",
    "result_df.sort_values(\"accuracy\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
